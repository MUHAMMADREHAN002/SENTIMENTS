# -*- coding: utf-8 -*-
"""Copy of Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZYXUc5bRrjzoavdTfn5kVZNVxeij9Q_Z
"""

pip install transformers datasets torch pandas numpy sklearn
pip install accelerate -U  # Required for Trainer
pip install streamlit
from transformers import pipeline
import numpy as np
from sklearn.metrics import accuracy_score
import streamlit as st
import pandas as pd
from sklearn.model_selection import train_test_split

import streamlit as st

uploaded_file = st.file_uploader("D:\documents.csv", type="csv")

if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)
    st.write(df.head())

# Load your dataset
df = pd.read_csv("/content/Sentiment Analysis.csv")  # Replace with your file path

# Check column names (ensure they match your CSV)
print(df.columns)  # Expected: ['Sr No', 'reviews', 'label']
# Extract texts and labels
texts = df["review"].tolist()
labels = df["labels"].tolist()  # Ensure labels are integers (0-4)

# Split into train/validation (80-20)
train_texts, val_texts, train_labels, val_labels = train_test_split(
    texts, labels, test_size=0.2, random_state=42
)

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment-latest")

# Tokenize function
def tokenize_function(examples):
    return tokenizer(examples, padding="max_length", truncation=True, max_length=128)

# Apply tokenization
train_encodings = tokenize_function(train_texts)
val_encodings = tokenize_function(val_texts)

import torch
from torch.utils.data import Dataset

class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = SentimentDataset(train_encodings, train_labels)
val_dataset = SentimentDataset(val_encodings, val_labels)

from transformers import AutoModelForSequenceClassification, AutoConfig

# Load config and modify num_labels
config = AutoConfig.from_pretrained(
    "cardiffnlp/twitter-roberta-base-sentiment-latest",
    num_labels=5  # Set to 5 classes
)

# Initialize model with random classifier weights
model = AutoModelForSequenceClassification.from_pretrained(
    "cardiffnlp/twitter-roberta-base-sentiment-latest",
    config=config,
    ignore_mismatched_sizes=True  # Allow size mismatch for the classifier
)

# Now you need to fine-tune this model on your 5-class dataset!

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

# Load model (with 5 classes)
model = AutoModelForSequenceClassification.from_pretrained(
    "cardiffnlp/twitter-roberta-base-sentiment-latest",
    num_labels=5,
    ignore_mismatched_sizes=True  # Required for resizing the classifier
)

# Convert string labels to integers (if needed)
label_map = {"very good": 0, "good": 1, "neutral": 2, "bad": 3, "very bad": 4}
train_dataset.labels = [label_map[label] for label in train_dataset.labels]
val_dataset.labels = [label_map[label] for label in val_dataset.labels]

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    load_best_model_at_end=True,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Train
trainer.train()

# Evaluate
results = trainer.evaluate()
print(f"Validation Accuracy: {results['eval_loss']}")

# Predict on new text
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)
    outputs = model(**inputs)
    predicted_class = torch.argmax(outputs.logits).item()
    label_map = {0: "very good", 1: "good", 2: "neutral", 3: "bad", 4: "very bad"}
    return label_map[predicted_class]

# Example prediction
print(predict_sentiment("I hate Facebook"))
print(predict_sentiment("Worst experience ever."))

# Save model and tokenizer
model.save_pretrained("./fine_tuned_roberta_5class")
tokenizer.save_pretrained("./fine_tuned_roberta_5class")

# Load later for inference
from transformers import pipeline

sentiment_analyzer = pipeline(
    "text-classification",
    model="./fine_tuned_roberta_5class",
    tokenizer="./fine_tuned_roberta_5class"
)

print(sentiment_analyzer("The service was okay."))  # Output: {'label': 'neutral', 'score': 0.92}

# Step 1: Install Required Libraries
!pip install pandas matplotlib seaborn textblob

# Step 2: Import Libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob

# Step 3: Load Your Data
# For demonstration, let's create a sample DataFrame
data = {
    'text': [
        'I love this product!',
        'This is the worst experience I have ever had.',
        'I am very happy with the service.',
        'I will never buy this again.',
        'Absolutely fantastic! Highly recommend it.'
    ]
}
df = pd.DataFrame(data)

# Step 4: Perform Sentiment Analysis
def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity  # Returns a value between -1 (negative) and 1 (positive)

df['sentiment'] = df['text'].apply(get_sentiment)

# Step 5: Visualize the Results
plt.figure(figsize=(10, 6))
sns.histplot(df['sentiment'], bins=10, kde=True)
plt.title('Sentiment Analysis Distribution')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')  # Line at 0 for neutral sentiment
plt.show()

